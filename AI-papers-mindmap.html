<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.1-alpha.4/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:q,mm:v}=window,j=new q.Toolbar;j.attach(v);const we=j.render();we.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(we)})})()</script><script>((f,d,h,u)=>{const g=f();window.mm=g.Markmap.create("svg#mindmap",(d||g.deriveOptions)(u),h)})(()=>window.markmap,null,{"content":"🧠 LLM Reasoning and Self-Improvement","children":[{"content":"🚀 Scaling Inference Compute","children":[{"content":"🧪 Repeated Sampling","children":[{"content":"💯 Coverage: Fraction of problems solvable with any sample.","children":[{"content":"🤔 Impact of Model Size: Smaller models can have higher coverage with more samples.","children":[],"payload":{"lines":"13,14"}},{"content":"📈 Scaling with Sample Count: Log-linear relationship for many models.","children":[],"payload":{"lines":"14,15"}}],"payload":{"lines":"12,15"}},{"content":"🎯 Precision: Ability to identify correct samples.","children":[{"content":"✅ Automatic Verifiers (Proof Checkers, Unit Tests)","children":[{"content":"🐞 False Positives: Correct answers with incorrect reasoning.","children":[],"payload":{"lines":"17,18"}},{"content":"🧪 CodeContests, MiniF2F-MATH, SWE-bench Lite","children":[],"payload":{"lines":"18,19"}}],"payload":{"lines":"16,19"}},{"content":"🤔 Domains Without Verifiers (GSM8K, MATH)","children":[{"content":"🗳️ Majority Voting: Plateaus beyond a certain sample count.","children":[],"payload":{"lines":"20,21"}},{"content":"🤖 Reward Model Scoring: Limited effectiveness in identifying rare correct samples.","children":[],"payload":{"lines":"21,23"}}],"payload":{"lines":"19,23"}}],"payload":{"lines":"15,23"}}],"payload":{"lines":"11,12"}},{"content":"💰 Cost-Effectiveness","children":[{"content":"⚖️ Trading Off Model Size and Sample Count:","children":[{"content":"💪 Larger models can be better for some tasks (e.g., CodeContests).","children":[],"payload":{"lines":"25,26"}},{"content":"🧠 Smaller models can be better for others (e.g., MiniF2F, GSM8K, MATH).","children":[],"payload":{"lines":"26,27"}}],"payload":{"lines":"24,27"}},{"content":"💸 API Costs: Cheaper models with more samples can be more cost-effective than expensive models with fewer samples (e.g., DeepSeek vs. GPT-4o, Claude).","children":[],"payload":{"lines":"27,29"}}],"payload":{"lines":"23,24"}}],"payload":{"lines":"9,10"}},{"content":"🤔 Enhancing Reasoning with Search","children":[{"content":"🌳 Tree Search Algorithms","children":[{"content":"🗺️ Exploring Solution Space: Traversing a tree of possible reasoning steps.","children":[{"content":"🧭 Guiding Search: Using value functions, reward models, and heuristics.","children":[],"payload":{"lines":"33,34"}},{"content":"⚖️ Balancing Exploration and Exploitation: UCT, PUCT algorithms.","children":[],"payload":{"lines":"34,35"}}],"payload":{"lines":"32,35"}},{"content":"🤖 Variants:","children":[{"content":"breadth-first search (BFS)","children":[],"payload":{"lines":"36,37"}},{"content":"🥇 depth-first search (DFS)","children":[],"payload":{"lines":"37,38"}},{"content":"🎲 Monte Carlo Tree Search (MCTS)","children":[{"content":"🔄 AlphaZero-like MCTS for LLM training and inference (TS-LLM).","children":[],"payload":{"lines":"39,40"}},{"content":"🧠 MCTS with a learned value function (MCTS-α).","children":[],"payload":{"lines":"40,41"}},{"content":"🚀 MCTS with rollouts and value function approximation (MCTS-Rollout).","children":[],"payload":{"lines":"41,42"}}],"payload":{"lines":"38,42"}},{"content":"⭐️ A* search.","children":[{"content":"🤔 Q* with a learned Q-value model as the heuristic function.","children":[],"payload":{"lines":"43,44"}},{"content":"🚀 REward BAlanced SEarch (REBASE) for efficient exploration and voting.","children":[],"payload":{"lines":"44,46"}}],"payload":{"lines":"42,46"}}],"payload":{"lines":"35,46"}}],"payload":{"lines":"31,32"}},{"content":"🧠 Chain of Thought (CoT)","children":[{"content":"🗣️ Rationale Generation: Generating a sequence of intermediate reasoning steps in natural language.","children":[{"content":"📈 Improves Performance: Especially effective for multi-step reasoning tasks.","children":[],"payload":{"lines":"48,49"}},{"content":"🔬 Emergent Ability: Becomes more effective with larger models.","children":[],"payload":{"lines":"49,50"}}],"payload":{"lines":"47,50"}},{"content":"🤔 Variations:","children":[{"content":"🗳️ Self-Consistency (CoT-SC): Sampling multiple CoT chains and using majority voting.","children":[],"payload":{"lines":"51,52"}},{"content":"❓ Least-to-Most Prompting: Decomposing problems into simpler sub-questions.","children":[],"payload":{"lines":"52,54"}}],"payload":{"lines":"50,54"}}],"payload":{"lines":"46,47"}},{"content":"🕵️ Verifiers and Reward Models","children":[{"content":"🤖 Guiding Search: Using learned models to evaluate reasoning steps and guide search.","children":[{"content":"🏁 Outcome-Based Verifiers (ORMs): Evaluating only the final answer.","children":[],"payload":{"lines":"56,57"}},{"content":"👣 Process-Based Verifiers (PRMs): Evaluating each step in the reasoning process.","children":[],"payload":{"lines":"57,58"}},{"content":"🧠 Process Reward Models (PRMs): Predicting step-level rewards based on correctness or quality.","children":[],"payload":{"lines":"58,59"}}],"payload":{"lines":"55,59"}},{"content":"🔎 Search Methods:","children":[{"content":"💯 Best-of-N: Sampling N complete solutions and selecting the highest-scoring one.","children":[],"payload":{"lines":"60,61"}},{"content":"🥇 Beam Search: Exploring a beam of promising partial solutions guided by verifier scores.","children":[],"payload":{"lines":"61,62"}},{"content":"🚀 Lookahead Search: Using rollouts to improve the accuracy of step-level evaluations.","children":[],"payload":{"lines":"62,64"}}],"payload":{"lines":"59,64"}}],"payload":{"lines":"54,55"}}],"payload":{"lines":"29,30"}},{"content":"🧠 Refining the Proposal Distribution","children":[{"content":"✍️ Revisions","children":[{"content":"🔄 Iterative Refinement: Generating a sequence of revisions to an initial answer.","children":[{"content":"🧠 Learning from Mistakes: Conditioning each revision on previous attempts and feedback.","children":[],"payload":{"lines":"68,69"}},{"content":"🤖 Revision Models: Fine-tuned to generate revisions based on feedback or critique.","children":[],"payload":{"lines":"69,70"}}],"payload":{"lines":"67,70"}},{"content":"⚖️ Trade-off:","children":[{"content":"🔀 Sequential Revisions: Generating revisions in a sequence.","children":[],"payload":{"lines":"71,72"}},{"content":"🎲 Parallel Revisions: Generating revisions independently.","children":[],"payload":{"lines":"72,73"}},{"content":"🤔 Optimal Ratio: Depends on problem difficulty and compute budget.","children":[],"payload":{"lines":"73,75"}}],"payload":{"lines":"70,75"}}],"payload":{"lines":"66,67"}},{"content":"📚 Procedure Cloning","children":[{"content":"🧠 Chain of Thought Imitation: Learning to imitate the reasoning process of expert procedures.","children":[{"content":"🤖 Procedure: A sequence of computations leading to a final action.","children":[],"payload":{"lines":"77,78"}},{"content":"📋 Procedure Observations: Intermediate computation states captured during procedure execution.","children":[],"payload":{"lines":"78,79"}}],"payload":{"lines":"76,79"}},{"content":"🎯 Procedure Cloning Objective: Maximizing the likelihood of procedure observations and actions.","children":[],"payload":{"lines":"79,80"}},{"content":"🚀 Benefits:","children":[{"content":"🌎 Generalization to unseen environments.","children":[],"payload":{"lines":"81,82"}},{"content":"🧠 Learning \"how\" and \"why\" in addition to \"what\" to do.","children":[],"payload":{"lines":"82,84"}}],"payload":{"lines":"80,84"}}],"payload":{"lines":"75,76"}},{"content":"💭 Uncertainty of Thoughts (UoT)","children":[{"content":"❓ Active Information Seeking: Asking questions to reduce uncertainty.","children":[{"content":"🌎 Possibility Space (Ω): The set of all possible options.","children":[],"payload":{"lines":"86,87"}},{"content":"🧠 Current Possibility Set (Ωi): Subset of Ω consistent with past answers.","children":[],"payload":{"lines":"87,88"}}],"payload":{"lines":"85,88"}},{"content":"🌳 Uncertainty-Aware Simulation:","children":[{"content":"🤔 Simulating possible future scenarios (answer trees).","children":[],"payload":{"lines":"89,90"}},{"content":"🧮 Computing uncertainty-based rewards (motivated by information gain).","children":[],"payload":{"lines":"90,91"}}],"payload":{"lines":"88,91"}},{"content":"🚀 Question Selection: Selecting questions to maximize expected information gain.","children":[{"content":"➕ Accumulated Reward (Ra): Sum of rewards at a node and its ancestors.","children":[],"payload":{"lines":"92,93"}},{"content":"🔮 Expected Reward (Re): Expected total reward from a node and its descendants.","children":[],"payload":{"lines":"93,95"}}],"payload":{"lines":"91,95"}}],"payload":{"lines":"84,85"}}],"payload":{"lines":"64,65"}},{"content":"🧠 Improving LLMs with Self-Training","children":[{"content":"📚 Self-Training Paradigms","children":[{"content":"🤖 Synthetic Data Generation: Using LLMs to generate training data for themselves or other LLMs.","children":[{"content":"💪 Stronger but Expensive (SE) Models: High-quality data, but limited sample count.","children":[],"payload":{"lines":"99,100"}},{"content":"🧠 Weaker but Cheaper (WC) Models: Higher coverage and diversity, but potentially higher false positive rate.","children":[],"payload":{"lines":"100,101"}}],"payload":{"lines":"98,101"}},{"content":"🔄 Iterative Training: Fine-tuning LLMs on self-generated data, then using the improved model to generate new data.","children":[],"payload":{"lines":"101,102"}},{"content":"🚀 Objectives:","children":[{"content":"🧠 Improving Reasoning Abilities: Training on synthetic solutions to challenging problems.","children":[],"payload":{"lines":"103,104"}},{"content":"🤝 Aligning with Human Preferences: Training on self-evaluated or curated data.","children":[],"payload":{"lines":"104,106"}}],"payload":{"lines":"102,106"}}],"payload":{"lines":"97,98"}},{"content":"🏆 Self-Rewarding Language Models","children":[{"content":"🧠 LLM-as-a-Judge: Using LLMs to evaluate their own generations.","children":[{"content":"❓ Prompting LLMs to score or rank responses.","children":[],"payload":{"lines":"108,109"}},{"content":"🧠 Learning to evaluate quality and correctness of text.","children":[],"payload":{"lines":"109,110"}}],"payload":{"lines":"107,110"}},{"content":"🔄 Iterative Training:","children":[{"content":"📈 Improving Instruction Following: Learning to generate better responses.","children":[],"payload":{"lines":"111,112"}},{"content":"🤖 Improving Reward Modeling: Learning to provide more accurate self-rewards.","children":[],"payload":{"lines":"112,113"}}],"payload":{"lines":"110,113"}},{"content":"🚀 Benefits:","children":[{"content":"🧑‍🤝‍🧑 Removing Reliance on Human Annotations: Bootstrapping reasoning and alignment from limited human data.","children":[],"payload":{"lines":"114,115"}},{"content":"📈 Potential for Continuous Improvement: Allowing models to improve their own training signal.","children":[],"payload":{"lines":"115,117"}}],"payload":{"lines":"113,117"}}],"payload":{"lines":"106,107"}}],"payload":{"lines":"95,96"}},{"content":"✅ Verifiers and Reward Models","children":[{"content":"🕵️ Types of Verifiers","children":[{"content":"🏁 Outcome-Based Verifiers (ORMs): Evaluating the correctness or quality of the final answer.","children":[],"payload":{"lines":"120,121"}},{"content":"👣 Process-Based Verifiers (PRMs): Evaluating each step in the reasoning process, typically assigning a score or label to each step.","children":[],"payload":{"lines":"121,123"}}],"payload":{"lines":"119,120"}},{"content":"🤖 Training Verifiers","children":[{"content":"🧑‍🏫 Human Feedback: Using human-annotated data to train verifiers.","children":[{"content":"💯 Correctness Labels: Binary labels indicating whether a solution is correct.","children":[],"payload":{"lines":"125,126"}},{"content":"🥇 Preference Rankings: Rankings of multiple solutions based on quality or preference.","children":[],"payload":{"lines":"126,127"}}],"payload":{"lines":"124,127"}},{"content":"🤖 Self-Supervision: Using data generated by the model itself to train verifiers.","children":[{"content":"🎲 Monte Carlo Rollouts: Simulating possible futures from each step and using the observed rewards to train a verifier.","children":[],"payload":{"lines":"128,129"}}],"payload":{"lines":"127,129"}},{"content":"🗣️ Synthetic Data from LLMs:","children":[{"content":"🚀 Generating a dataset of solutions from a stronger LLM, then using a weaker LLM to learn to verify those solutions.","children":[],"payload":{"lines":"130,131"}},{"content":"🧠 Using a model to generate its own solutions and verification rationales.","children":[],"payload":{"lines":"131,133"}}],"payload":{"lines":"129,133"}}],"payload":{"lines":"123,124"}},{"content":"🚀 Generative Verifiers (GenRM)","children":[{"content":"🧠 Next-Token Prediction: Training verifiers using the standard next-token prediction objective, treating the verification decision as a generated token.","children":[],"payload":{"lines":"134,135"}},{"content":"🗣️ Leveraging LLM Capabilities:","children":[{"content":"🤔 Chain-of-Thought (CoT): Generating intermediate reasoning steps to justify the verification decision.","children":[],"payload":{"lines":"136,137"}},{"content":"🗳️ Majority Voting: Sampling multiple CoT rationales and using majority voting to improve accuracy.","children":[],"payload":{"lines":"137,138"}}],"payload":{"lines":"135,138"}},{"content":"🤝 Unification: Training the same model to perform both solution generation and verification using different prompts.","children":[],"payload":{"lines":"138,140"}}],"payload":{"lines":"133,134"}}],"payload":{"lines":"117,118"}},{"content":"🔑 Concepts and Insights","children":[{"content":"💡 Emergence of Reasoning Abilities","children":[{"content":"📈 Model Scale: Reasoning performance generally improves as models scale in size and training compute.","children":[],"payload":{"lines":"143,144"}},{"content":"🧠 Emergent Abilities: Certain reasoning abilities may only emerge in larger models, not being present or predictable in smaller ones.","children":[],"payload":{"lines":"144,146"}}],"payload":{"lines":"142,143"}},{"content":"🤔 Credit Assignment Problem","children":[{"content":"🔄 Challenge: Determining which reasoning steps contribute to the success or failure of a solution.","children":[],"payload":{"lines":"147,148"}},{"content":"👣 Solution: Process supervision (using PRMs) helps with credit assignment by providing feedback on each step.","children":[],"payload":{"lines":"148,150"}}],"payload":{"lines":"146,147"}},{"content":"🤝 Alignment Tax","children":[{"content":"🛡️ Cost:  The potential performance penalty incurred when aligning AI systems with human values.","children":[],"payload":{"lines":"151,152"}},{"content":"📉 Negative Alignment Tax: Some alignment techniques can improve performance in addition to improving alignment (e.g., process supervision).","children":[],"payload":{"lines":"152,154"}}],"payload":{"lines":"150,151"}},{"content":"🧬 Data Scaling and Diversity","children":[{"content":"📈 Impact: The size and diversity of training data significantly affects reasoning performance.","children":[],"payload":{"lines":"155,156"}},{"content":"🤖 Synthetic Data: Generating synthetic data from LLMs can overcome limitations of human-annotated data.","children":[{"content":"💪 Stronger but Expensive (SE) Models: Generate higher-quality data, but fewer samples for a given budget.","children":[],"payload":{"lines":"157,158"}},{"content":"🧠 Weaker but Cheaper (WC) Models: Can generate more diverse and potentially higher-coverage data.","children":[],"payload":{"lines":"158,160"}}],"payload":{"lines":"156,160"}}],"payload":{"lines":"154,155"}},{"content":"📊 Inference vs. Pretraining Compute Trade-off","children":[{"content":"🚀 Exchange Rate: It may be possible to trade off pretraining compute for inference compute to achieve a desired performance level.","children":[{"content":"🧠 Easy Problems: Test-time compute can be more effective than scaling model parameters.","children":[],"payload":{"lines":"162,163"}},{"content":"💪 Hard Problems: Pretraining compute may be more effective, especially with limited inference budgets.","children":[],"payload":{"lines":"163,164"}},{"content":"🌎 Task and Model Dependence: The optimal allocation of compute depends on the specific task and model.","children":[],"payload":{"lines":"164,166"}}],"payload":{"lines":"161,166"}}],"payload":{"lines":"160,161"}}],"payload":{"lines":"140,141"}},{"content":"⚒️ Tools and Techniques","children":[{"content":"🤖 Monte Carlo Tree Search (MCTS)","children":[{"content":"🌳 Exploration:  Exploring the solution space by simulating multiple possible future scenarios (rollouts) from each node.","children":[],"payload":{"lines":"169,170"}},{"content":"🧠 Guidance: Using value estimates, reward models, and heuristics to guide the search towards promising nodes.","children":[],"payload":{"lines":"170,171"}},{"content":"🚀 Exploitation vs. Exploration: Balancing the trade-off between exploiting high-value nodes and exploring less-visited ones.","children":[],"payload":{"lines":"171,172"}},{"content":"🤔 Variants:","children":[{"content":"💯 UCT: Upper Confidence Bound applied to Trees, a common strategy for action selection in MCTS.","children":[],"payload":{"lines":"173,174"}},{"content":"🚀 PUCT: Predictor + UCT, incorporates a prior policy distribution into the UCT formula.","children":[],"payload":{"lines":"174,176"}}],"payload":{"lines":"172,176"}}],"payload":{"lines":"168,169"}},{"content":"🎯 Direct Preference Optimization (DPO)","children":[{"content":"🥇 Preference Learning: Training models directly from human-provided preferences between different outputs.","children":[],"payload":{"lines":"177,178"}},{"content":"🧠 Implicit Reward Modeling: Representing the reward implicitly using the logits of the policy model.","children":[],"payload":{"lines":"178,179"}},{"content":"🔄 Iterative DPO: Iteratively updating the policy model based on new preference data.","children":[],"payload":{"lines":"179,180"}},{"content":"🤔 Limitations:","children":[{"content":"📉 Potential for Overfitting: Can overfit to the initial preference data, leading to performance degradation.","children":[],"payload":{"lines":"181,182"}},{"content":"🤖 Difficulty with Reasoning Tasks: May struggle with reasoning tasks where the space of correct answers is small.","children":[],"payload":{"lines":"182,184"}}],"payload":{"lines":"180,184"}}],"payload":{"lines":"176,177"}},{"content":"🔑 Meta-Tokens","children":[{"content":"🧠 Controllable Language Modeling: Using learnable tokens to control specific aspects of model behavior (e.g., thought tokens).","children":[],"payload":{"lines":"185,186"}},{"content":"🗣️ Learned Symbols: Training models to learn the meaning and function of meta-tokens, enabling more flexible and adaptable language modeling.","children":[],"payload":{"lines":"186,188"}}],"payload":{"lines":"184,185"}},{"content":"🤖 External Tools","children":[{"content":"🧮 Tool Integration: Using calculators, code interpreters, symbolic solvers, etc. to enhance LLM capabilities.","children":[],"payload":{"lines":"189,190"}},{"content":"🚀 Code as Action: Using code generation and execution as actions in a reasoning process.","children":[],"payload":{"lines":"190,191"}},{"content":"🧠 Knowledge Augmentation: Integrating external knowledge sources (e.g., knowledge bases, APIs) into LLMs.","children":[],"payload":{"lines":"191,193"}}],"payload":{"lines":"188,189"}},{"content":"🕵️ Active Learning","children":[{"content":"🎯 Data Selection: Selectively choosing the most informative data for human feedback, reducing annotation cost and improving model alignment.","children":[],"payload":{"lines":"194,195"}},{"content":"🤔 Convincing Wrong Answers: Prioritizing the labeling of solutions that are highly scored by a reward model but lead to incorrect answers.","children":[],"payload":{"lines":"195,197"}}],"payload":{"lines":"193,194"}},{"content":"🧪 Out-of-Distribution (OOD) Generalization","children":[{"content":"🌎 Unseen Data: Evaluating model performance on data that is different from the training distribution.","children":[],"payload":{"lines":"198,199"}},{"content":"🚀 Transfer Learning: Measuring the ability of models to transfer their reasoning skills to new domains or tasks.","children":[],"payload":{"lines":"199,200"}},{"content":"💪 Benchmarks: Evaluating on recent or challenging benchmarks to assess true capabilities.","children":[],"payload":{"lines":"200,202"}}],"payload":{"lines":"197,198"}}],"payload":{"lines":"166,167"}}],"payload":{"lines":"7,8"}},{"initialExpandLevel":6,"maxWidth":null})</script>
</body>
</html>
