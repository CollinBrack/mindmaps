<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.1-alpha.4/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:q,mm:v}=window,j=new q.Toolbar;j.attach(v);const we=j.render();we.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(we)})})()</script><script>((f,d,h,u)=>{const g=f();window.mm=g.Markmap.create("svg#mindmap",(d||g.deriveOptions)(u),h)})(()=>window.markmap,null,{"content":"ğŸ§  LLM Reasoning and Self-Improvement","children":[{"content":"ğŸš€ Scaling Inference Compute","children":[{"content":"ğŸ§ª Repeated Sampling","children":[{"content":"ğŸ’¯ Coverage: Fraction of problems solvable with any sample.","children":[{"content":"ğŸ¤” Impact of Model Size: Smaller models can have higher coverage with more samples.","children":[],"payload":{"lines":"13,14"}},{"content":"ğŸ“ˆ Scaling with Sample Count: Log-linear relationship for many models.","children":[],"payload":{"lines":"14,15"}}],"payload":{"lines":"12,15"}},{"content":"ğŸ¯ Precision: Ability to identify correct samples.","children":[{"content":"âœ… Automatic Verifiers (Proof Checkers, Unit Tests)","children":[{"content":"ğŸ False Positives: Correct answers with incorrect reasoning.","children":[],"payload":{"lines":"17,18"}},{"content":"ğŸ§ª CodeContests, MiniF2F-MATH, SWE-bench Lite","children":[],"payload":{"lines":"18,19"}}],"payload":{"lines":"16,19"}},{"content":"ğŸ¤” Domains Without Verifiers (GSM8K, MATH)","children":[{"content":"ğŸ—³ï¸ Majority Voting: Plateaus beyond a certain sample count.","children":[],"payload":{"lines":"20,21"}},{"content":"ğŸ¤– Reward Model Scoring: Limited effectiveness in identifying rare correct samples.","children":[],"payload":{"lines":"21,23"}}],"payload":{"lines":"19,23"}}],"payload":{"lines":"15,23"}}],"payload":{"lines":"11,12"}},{"content":"ğŸ’° Cost-Effectiveness","children":[{"content":"âš–ï¸ Trading Off Model Size and Sample Count:","children":[{"content":"ğŸ’ª Larger models can be better for some tasks (e.g., CodeContests).","children":[],"payload":{"lines":"25,26"}},{"content":"ğŸ§  Smaller models can be better for others (e.g., MiniF2F, GSM8K, MATH).","children":[],"payload":{"lines":"26,27"}}],"payload":{"lines":"24,27"}},{"content":"ğŸ’¸ API Costs: Cheaper models with more samples can be more cost-effective than expensive models with fewer samples (e.g., DeepSeek vs. GPT-4o, Claude).","children":[],"payload":{"lines":"27,29"}}],"payload":{"lines":"23,24"}}],"payload":{"lines":"9,10"}},{"content":"ğŸ¤” Enhancing Reasoning with Search","children":[{"content":"ğŸŒ³ Tree Search Algorithms","children":[{"content":"ğŸ—ºï¸ Exploring Solution Space: Traversing a tree of possible reasoning steps.","children":[{"content":"ğŸ§­ Guiding Search: Using value functions, reward models, and heuristics.","children":[],"payload":{"lines":"33,34"}},{"content":"âš–ï¸ Balancing Exploration and Exploitation: UCT, PUCT algorithms.","children":[],"payload":{"lines":"34,35"}}],"payload":{"lines":"32,35"}},{"content":"ğŸ¤– Variants:","children":[{"content":"breadth-first search (BFS)","children":[],"payload":{"lines":"36,37"}},{"content":"ğŸ¥‡ depth-first search (DFS)","children":[],"payload":{"lines":"37,38"}},{"content":"ğŸ² Monte Carlo Tree Search (MCTS)","children":[{"content":"ğŸ”„ AlphaZero-like MCTS for LLM training and inference (TS-LLM).","children":[],"payload":{"lines":"39,40"}},{"content":"ğŸ§  MCTS with a learned value function (MCTS-Î±).","children":[],"payload":{"lines":"40,41"}},{"content":"ğŸš€ MCTS with rollouts and value function approximation (MCTS-Rollout).","children":[],"payload":{"lines":"41,42"}}],"payload":{"lines":"38,42"}},{"content":"â­ï¸ A* search.","children":[{"content":"ğŸ¤” Q* with a learned Q-value model as the heuristic function.","children":[],"payload":{"lines":"43,44"}},{"content":"ğŸš€ REward BAlanced SEarch (REBASE) for efficient exploration and voting.","children":[],"payload":{"lines":"44,46"}}],"payload":{"lines":"42,46"}}],"payload":{"lines":"35,46"}}],"payload":{"lines":"31,32"}},{"content":"ğŸ§  Chain of Thought (CoT)","children":[{"content":"ğŸ—£ï¸ Rationale Generation: Generating a sequence of intermediate reasoning steps in natural language.","children":[{"content":"ğŸ“ˆ Improves Performance: Especially effective for multi-step reasoning tasks.","children":[],"payload":{"lines":"48,49"}},{"content":"ğŸ”¬ Emergent Ability: Becomes more effective with larger models.","children":[],"payload":{"lines":"49,50"}}],"payload":{"lines":"47,50"}},{"content":"ğŸ¤” Variations:","children":[{"content":"ğŸ—³ï¸ Self-Consistency (CoT-SC): Sampling multiple CoT chains and using majority voting.","children":[],"payload":{"lines":"51,52"}},{"content":"â“ Least-to-Most Prompting: Decomposing problems into simpler sub-questions.","children":[],"payload":{"lines":"52,54"}}],"payload":{"lines":"50,54"}}],"payload":{"lines":"46,47"}},{"content":"ğŸ•µï¸ Verifiers and Reward Models","children":[{"content":"ğŸ¤– Guiding Search: Using learned models to evaluate reasoning steps and guide search.","children":[{"content":"ğŸ Outcome-Based Verifiers (ORMs): Evaluating only the final answer.","children":[],"payload":{"lines":"56,57"}},{"content":"ğŸ‘£ Process-Based Verifiers (PRMs): Evaluating each step in the reasoning process.","children":[],"payload":{"lines":"57,58"}},{"content":"ğŸ§  Process Reward Models (PRMs): Predicting step-level rewards based on correctness or quality.","children":[],"payload":{"lines":"58,59"}}],"payload":{"lines":"55,59"}},{"content":"ğŸ” Search Methods:","children":[{"content":"ğŸ’¯ Best-of-N: Sampling N complete solutions and selecting the highest-scoring one.","children":[],"payload":{"lines":"60,61"}},{"content":"ğŸ¥‡ Beam Search: Exploring a beam of promising partial solutions guided by verifier scores.","children":[],"payload":{"lines":"61,62"}},{"content":"ğŸš€ Lookahead Search: Using rollouts to improve the accuracy of step-level evaluations.","children":[],"payload":{"lines":"62,64"}}],"payload":{"lines":"59,64"}}],"payload":{"lines":"54,55"}}],"payload":{"lines":"29,30"}},{"content":"ğŸ§  Refining the Proposal Distribution","children":[{"content":"âœï¸ Revisions","children":[{"content":"ğŸ”„ Iterative Refinement: Generating a sequence of revisions to an initial answer.","children":[{"content":"ğŸ§  Learning from Mistakes: Conditioning each revision on previous attempts and feedback.","children":[],"payload":{"lines":"68,69"}},{"content":"ğŸ¤– Revision Models: Fine-tuned to generate revisions based on feedback or critique.","children":[],"payload":{"lines":"69,70"}}],"payload":{"lines":"67,70"}},{"content":"âš–ï¸ Trade-off:","children":[{"content":"ğŸ”€ Sequential Revisions: Generating revisions in a sequence.","children":[],"payload":{"lines":"71,72"}},{"content":"ğŸ² Parallel Revisions: Generating revisions independently.","children":[],"payload":{"lines":"72,73"}},{"content":"ğŸ¤” Optimal Ratio: Depends on problem difficulty and compute budget.","children":[],"payload":{"lines":"73,75"}}],"payload":{"lines":"70,75"}}],"payload":{"lines":"66,67"}},{"content":"ğŸ“š Procedure Cloning","children":[{"content":"ğŸ§  Chain of Thought Imitation: Learning to imitate the reasoning process of expert procedures.","children":[{"content":"ğŸ¤– Procedure: A sequence of computations leading to a final action.","children":[],"payload":{"lines":"77,78"}},{"content":"ğŸ“‹ Procedure Observations: Intermediate computation states captured during procedure execution.","children":[],"payload":{"lines":"78,79"}}],"payload":{"lines":"76,79"}},{"content":"ğŸ¯ Procedure Cloning Objective: Maximizing the likelihood of procedure observations and actions.","children":[],"payload":{"lines":"79,80"}},{"content":"ğŸš€ Benefits:","children":[{"content":"ğŸŒ Generalization to unseen environments.","children":[],"payload":{"lines":"81,82"}},{"content":"ğŸ§  Learning \"how\" and \"why\" in addition to \"what\" to do.","children":[],"payload":{"lines":"82,84"}}],"payload":{"lines":"80,84"}}],"payload":{"lines":"75,76"}},{"content":"ğŸ’­ Uncertainty of Thoughts (UoT)","children":[{"content":"â“ Active Information Seeking: Asking questions to reduce uncertainty.","children":[{"content":"ğŸŒ Possibility Space (Î©): The set of all possible options.","children":[],"payload":{"lines":"86,87"}},{"content":"ğŸ§  Current Possibility Set (Î©i): Subset of Î© consistent with past answers.","children":[],"payload":{"lines":"87,88"}}],"payload":{"lines":"85,88"}},{"content":"ğŸŒ³ Uncertainty-Aware Simulation:","children":[{"content":"ğŸ¤” Simulating possible future scenarios (answer trees).","children":[],"payload":{"lines":"89,90"}},{"content":"ğŸ§® Computing uncertainty-based rewards (motivated by information gain).","children":[],"payload":{"lines":"90,91"}}],"payload":{"lines":"88,91"}},{"content":"ğŸš€ Question Selection: Selecting questions to maximize expected information gain.","children":[{"content":"â• Accumulated Reward (Ra): Sum of rewards at a node and its ancestors.","children":[],"payload":{"lines":"92,93"}},{"content":"ğŸ”® Expected Reward (Re): Expected total reward from a node and its descendants.","children":[],"payload":{"lines":"93,95"}}],"payload":{"lines":"91,95"}}],"payload":{"lines":"84,85"}}],"payload":{"lines":"64,65"}},{"content":"ğŸ§  Improving LLMs with Self-Training","children":[{"content":"ğŸ“š Self-Training Paradigms","children":[{"content":"ğŸ¤– Synthetic Data Generation: Using LLMs to generate training data for themselves or other LLMs.","children":[{"content":"ğŸ’ª Stronger but Expensive (SE) Models: High-quality data, but limited sample count.","children":[],"payload":{"lines":"99,100"}},{"content":"ğŸ§  Weaker but Cheaper (WC) Models: Higher coverage and diversity, but potentially higher false positive rate.","children":[],"payload":{"lines":"100,101"}}],"payload":{"lines":"98,101"}},{"content":"ğŸ”„ Iterative Training: Fine-tuning LLMs on self-generated data, then using the improved model to generate new data.","children":[],"payload":{"lines":"101,102"}},{"content":"ğŸš€ Objectives:","children":[{"content":"ğŸ§  Improving Reasoning Abilities: Training on synthetic solutions to challenging problems.","children":[],"payload":{"lines":"103,104"}},{"content":"ğŸ¤ Aligning with Human Preferences: Training on self-evaluated or curated data.","children":[],"payload":{"lines":"104,106"}}],"payload":{"lines":"102,106"}}],"payload":{"lines":"97,98"}},{"content":"ğŸ† Self-Rewarding Language Models","children":[{"content":"ğŸ§  LLM-as-a-Judge: Using LLMs to evaluate their own generations.","children":[{"content":"â“ Prompting LLMs to score or rank responses.","children":[],"payload":{"lines":"108,109"}},{"content":"ğŸ§  Learning to evaluate quality and correctness of text.","children":[],"payload":{"lines":"109,110"}}],"payload":{"lines":"107,110"}},{"content":"ğŸ”„ Iterative Training:","children":[{"content":"ğŸ“ˆ Improving Instruction Following: Learning to generate better responses.","children":[],"payload":{"lines":"111,112"}},{"content":"ğŸ¤– Improving Reward Modeling: Learning to provide more accurate self-rewards.","children":[],"payload":{"lines":"112,113"}}],"payload":{"lines":"110,113"}},{"content":"ğŸš€ Benefits:","children":[{"content":"ğŸ§‘â€ğŸ¤â€ğŸ§‘ Removing Reliance on Human Annotations: Bootstrapping reasoning and alignment from limited human data.","children":[],"payload":{"lines":"114,115"}},{"content":"ğŸ“ˆ Potential for Continuous Improvement: Allowing models to improve their own training signal.","children":[],"payload":{"lines":"115,117"}}],"payload":{"lines":"113,117"}}],"payload":{"lines":"106,107"}}],"payload":{"lines":"95,96"}},{"content":"âœ… Verifiers and Reward Models","children":[{"content":"ğŸ•µï¸ Types of Verifiers","children":[{"content":"ğŸ Outcome-Based Verifiers (ORMs): Evaluating the correctness or quality of the final answer.","children":[],"payload":{"lines":"120,121"}},{"content":"ğŸ‘£ Process-Based Verifiers (PRMs): Evaluating each step in the reasoning process, typically assigning a score or label to each step.","children":[],"payload":{"lines":"121,123"}}],"payload":{"lines":"119,120"}},{"content":"ğŸ¤– Training Verifiers","children":[{"content":"ğŸ§‘â€ğŸ« Human Feedback: Using human-annotated data to train verifiers.","children":[{"content":"ğŸ’¯ Correctness Labels: Binary labels indicating whether a solution is correct.","children":[],"payload":{"lines":"125,126"}},{"content":"ğŸ¥‡ Preference Rankings: Rankings of multiple solutions based on quality or preference.","children":[],"payload":{"lines":"126,127"}}],"payload":{"lines":"124,127"}},{"content":"ğŸ¤– Self-Supervision: Using data generated by the model itself to train verifiers.","children":[{"content":"ğŸ² Monte Carlo Rollouts: Simulating possible futures from each step and using the observed rewards to train a verifier.","children":[],"payload":{"lines":"128,129"}}],"payload":{"lines":"127,129"}},{"content":"ğŸ—£ï¸ Synthetic Data from LLMs:","children":[{"content":"ğŸš€ Generating a dataset of solutions from a stronger LLM, then using a weaker LLM to learn to verify those solutions.","children":[],"payload":{"lines":"130,131"}},{"content":"ğŸ§  Using a model to generate its own solutions and verification rationales.","children":[],"payload":{"lines":"131,133"}}],"payload":{"lines":"129,133"}}],"payload":{"lines":"123,124"}},{"content":"ğŸš€ Generative Verifiers (GenRM)","children":[{"content":"ğŸ§  Next-Token Prediction: Training verifiers using the standard next-token prediction objective, treating the verification decision as a generated token.","children":[],"payload":{"lines":"134,135"}},{"content":"ğŸ—£ï¸ Leveraging LLM Capabilities:","children":[{"content":"ğŸ¤” Chain-of-Thought (CoT): Generating intermediate reasoning steps to justify the verification decision.","children":[],"payload":{"lines":"136,137"}},{"content":"ğŸ—³ï¸ Majority Voting: Sampling multiple CoT rationales and using majority voting to improve accuracy.","children":[],"payload":{"lines":"137,138"}}],"payload":{"lines":"135,138"}},{"content":"ğŸ¤ Unification: Training the same model to perform both solution generation and verification using different prompts.","children":[],"payload":{"lines":"138,140"}}],"payload":{"lines":"133,134"}}],"payload":{"lines":"117,118"}},{"content":"ğŸ”‘ Concepts and Insights","children":[{"content":"ğŸ’¡ Emergence of Reasoning Abilities","children":[{"content":"ğŸ“ˆ Model Scale: Reasoning performance generally improves as models scale in size and training compute.","children":[],"payload":{"lines":"143,144"}},{"content":"ğŸ§  Emergent Abilities: Certain reasoning abilities may only emerge in larger models, not being present or predictable in smaller ones.","children":[],"payload":{"lines":"144,146"}}],"payload":{"lines":"142,143"}},{"content":"ğŸ¤” Credit Assignment Problem","children":[{"content":"ğŸ”„ Challenge: Determining which reasoning steps contribute to the success or failure of a solution.","children":[],"payload":{"lines":"147,148"}},{"content":"ğŸ‘£ Solution: Process supervision (using PRMs) helps with credit assignment by providing feedback on each step.","children":[],"payload":{"lines":"148,150"}}],"payload":{"lines":"146,147"}},{"content":"ğŸ¤ Alignment Tax","children":[{"content":"ğŸ›¡ï¸ Cost:  The potential performance penalty incurred when aligning AI systems with human values.","children":[],"payload":{"lines":"151,152"}},{"content":"ğŸ“‰ Negative Alignment Tax: Some alignment techniques can improve performance in addition to improving alignment (e.g., process supervision).","children":[],"payload":{"lines":"152,154"}}],"payload":{"lines":"150,151"}},{"content":"ğŸ§¬ Data Scaling and Diversity","children":[{"content":"ğŸ“ˆ Impact: The size and diversity of training data significantly affects reasoning performance.","children":[],"payload":{"lines":"155,156"}},{"content":"ğŸ¤– Synthetic Data: Generating synthetic data from LLMs can overcome limitations of human-annotated data.","children":[{"content":"ğŸ’ª Stronger but Expensive (SE) Models: Generate higher-quality data, but fewer samples for a given budget.","children":[],"payload":{"lines":"157,158"}},{"content":"ğŸ§  Weaker but Cheaper (WC) Models: Can generate more diverse and potentially higher-coverage data.","children":[],"payload":{"lines":"158,160"}}],"payload":{"lines":"156,160"}}],"payload":{"lines":"154,155"}},{"content":"ğŸ“Š Inference vs. Pretraining Compute Trade-off","children":[{"content":"ğŸš€ Exchange Rate: It may be possible to trade off pretraining compute for inference compute to achieve a desired performance level.","children":[{"content":"ğŸ§  Easy Problems: Test-time compute can be more effective than scaling model parameters.","children":[],"payload":{"lines":"162,163"}},{"content":"ğŸ’ª Hard Problems: Pretraining compute may be more effective, especially with limited inference budgets.","children":[],"payload":{"lines":"163,164"}},{"content":"ğŸŒ Task and Model Dependence: The optimal allocation of compute depends on the specific task and model.","children":[],"payload":{"lines":"164,166"}}],"payload":{"lines":"161,166"}}],"payload":{"lines":"160,161"}}],"payload":{"lines":"140,141"}},{"content":"âš’ï¸ Tools and Techniques","children":[{"content":"ğŸ¤– Monte Carlo Tree Search (MCTS)","children":[{"content":"ğŸŒ³ Exploration:  Exploring the solution space by simulating multiple possible future scenarios (rollouts) from each node.","children":[],"payload":{"lines":"169,170"}},{"content":"ğŸ§  Guidance: Using value estimates, reward models, and heuristics to guide the search towards promising nodes.","children":[],"payload":{"lines":"170,171"}},{"content":"ğŸš€ Exploitation vs. Exploration: Balancing the trade-off between exploiting high-value nodes and exploring less-visited ones.","children":[],"payload":{"lines":"171,172"}},{"content":"ğŸ¤” Variants:","children":[{"content":"ğŸ’¯ UCT: Upper Confidence Bound applied to Trees, a common strategy for action selection in MCTS.","children":[],"payload":{"lines":"173,174"}},{"content":"ğŸš€ PUCT: Predictor + UCT, incorporates a prior policy distribution into the UCT formula.","children":[],"payload":{"lines":"174,176"}}],"payload":{"lines":"172,176"}}],"payload":{"lines":"168,169"}},{"content":"ğŸ¯ Direct Preference Optimization (DPO)","children":[{"content":"ğŸ¥‡ Preference Learning: Training models directly from human-provided preferences between different outputs.","children":[],"payload":{"lines":"177,178"}},{"content":"ğŸ§  Implicit Reward Modeling: Representing the reward implicitly using the logits of the policy model.","children":[],"payload":{"lines":"178,179"}},{"content":"ğŸ”„ Iterative DPO: Iteratively updating the policy model based on new preference data.","children":[],"payload":{"lines":"179,180"}},{"content":"ğŸ¤” Limitations:","children":[{"content":"ğŸ“‰ Potential for Overfitting: Can overfit to the initial preference data, leading to performance degradation.","children":[],"payload":{"lines":"181,182"}},{"content":"ğŸ¤– Difficulty with Reasoning Tasks: May struggle with reasoning tasks where the space of correct answers is small.","children":[],"payload":{"lines":"182,184"}}],"payload":{"lines":"180,184"}}],"payload":{"lines":"176,177"}},{"content":"ğŸ”‘ Meta-Tokens","children":[{"content":"ğŸ§  Controllable Language Modeling: Using learnable tokens to control specific aspects of model behavior (e.g., thought tokens).","children":[],"payload":{"lines":"185,186"}},{"content":"ğŸ—£ï¸ Learned Symbols: Training models to learn the meaning and function of meta-tokens, enabling more flexible and adaptable language modeling.","children":[],"payload":{"lines":"186,188"}}],"payload":{"lines":"184,185"}},{"content":"ğŸ¤– External Tools","children":[{"content":"ğŸ§® Tool Integration: Using calculators, code interpreters, symbolic solvers, etc. to enhance LLM capabilities.","children":[],"payload":{"lines":"189,190"}},{"content":"ğŸš€ Code as Action: Using code generation and execution as actions in a reasoning process.","children":[],"payload":{"lines":"190,191"}},{"content":"ğŸ§  Knowledge Augmentation: Integrating external knowledge sources (e.g., knowledge bases, APIs) into LLMs.","children":[],"payload":{"lines":"191,193"}}],"payload":{"lines":"188,189"}},{"content":"ğŸ•µï¸ Active Learning","children":[{"content":"ğŸ¯ Data Selection: Selectively choosing the most informative data for human feedback, reducing annotation cost and improving model alignment.","children":[],"payload":{"lines":"194,195"}},{"content":"ğŸ¤” Convincing Wrong Answers: Prioritizing the labeling of solutions that are highly scored by a reward model but lead to incorrect answers.","children":[],"payload":{"lines":"195,197"}}],"payload":{"lines":"193,194"}},{"content":"ğŸ§ª Out-of-Distribution (OOD) Generalization","children":[{"content":"ğŸŒ Unseen Data: Evaluating model performance on data that is different from the training distribution.","children":[],"payload":{"lines":"198,199"}},{"content":"ğŸš€ Transfer Learning: Measuring the ability of models to transfer their reasoning skills to new domains or tasks.","children":[],"payload":{"lines":"199,200"}},{"content":"ğŸ’ª Benchmarks: Evaluating on recent or challenging benchmarks to assess true capabilities.","children":[],"payload":{"lines":"200,202"}}],"payload":{"lines":"197,198"}}],"payload":{"lines":"166,167"}}],"payload":{"lines":"7,8"}},{"initialExpandLevel":6,"maxWidth":null})</script>
</body>
</html>
