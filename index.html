<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.1-alpha.4/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:q,mm:v}=window,j=new q.Toolbar;j.attach(v);const we=j.render();we.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(we)})})()</script><script>((f,d,h,u)=>{const g=f();window.mm=g.Markmap.create("svg#mindmap",(d||g.deriveOptions)(u),h)})(()=>window.markmap,null,{"content":"📚 Hacker's Guide to LLMs","children":[{"content":"🧠 Chapter 1: The Rise of LLMs: From Calculators to Creative Machines","children":[{"content":"From calculators to creative machines 🧮➡️🎨","children":[],"payload":{"lines":"5,6"}},{"content":"AI revolution 💥","children":[],"payload":{"lines":"6,7"}},{"content":"Transformer architecture 🤖","children":[],"payload":{"lines":"7,9"}}],"payload":{"lines":"3,4"}},{"content":"🧠 Chapter 2: Understanding the Transformer: Attention is All You Need","children":[{"content":"Attention is all you need 👀","children":[],"payload":{"lines":"11,12"}},{"content":"Input embeddings: words to numbers 🔤➡️🔢","children":[],"payload":{"lines":"12,13"}},{"content":"Positional encodings: preserving order 🔢➡️🗺️","children":[],"payload":{"lines":"13,14"}},{"content":"Self-attention: word relationships 🔗","children":[],"payload":{"lines":"14,15"}},{"content":"Multi-head attention: diverse perspectives 🧠🧠🧠","children":[],"payload":{"lines":"15,17"}}],"payload":{"lines":"9,10"}},{"content":"🏋️‍♀️ Chapter 3: The Training Pipeline: Pre-Training, Fine-Tuning, and Beyond","children":[{"content":"Pre-training:  ocean of data 🌊📚","children":[],"payload":{"lines":"19,20"}},{"content":"Fine-tuning: specific tasks 🎯","children":[],"payload":{"lines":"20,21"}},{"content":"Instruction tuning: following instructions 📝🤖","children":[],"payload":{"lines":"21,22"}},{"content":"RLHF: human feedback 🧑‍🏫🤖","children":[],"payload":{"lines":"22,24"}}],"payload":{"lines":"17,18"}},{"content":"✍️ Chapter 4: The Art of Prompt Engineering: Guiding Your LLM to Success","children":[{"content":"Guiding your LLM 🧑‍✈️🤖","children":[],"payload":{"lines":"26,27"}},{"content":"Clarity and conciseness 💎","children":[],"payload":{"lines":"27,28"}},{"content":"Specificity 🎯","children":[],"payload":{"lines":"28,29"}},{"content":"Context is key 🗺️","children":[],"payload":{"lines":"29,30"}},{"content":"Iterative refinement 🔄","children":[],"payload":{"lines":"30,32"}}],"payload":{"lines":"24,25"}},{"content":"🔌 Chapter 5: Interacting with OpenAI API: Accessing GPT Power Programmatically","children":[{"content":"Accessing GPT power 🧠⚡","children":[],"payload":{"lines":"34,35"}},{"content":"Programmatic prompts 🤖💻","children":[],"payload":{"lines":"35,36"}},{"content":"Function calling 🧰","children":[],"payload":{"lines":"36,37"}},{"content":"Fine-tuning 🛠️","children":[],"payload":{"lines":"37,38"}},{"content":"Embeddings 🧬","children":[],"payload":{"lines":"38,40"}}],"payload":{"lines":"32,33"}},{"content":"🧰 Chapter 6: Extending LLM Capabilities: Function Calling and Tool Integration","children":[{"content":"Function calling: language to action 💬➡️💪","children":[],"payload":{"lines":"42,43"}},{"content":"Tool integration 🔌🧰","children":[],"payload":{"lines":"43,44"}},{"content":"Real-time information ⚡️","children":[],"payload":{"lines":"44,45"}},{"content":"Data manipulation 🧮","children":[],"payload":{"lines":"45,46"}},{"content":"Complex actions 🚀","children":[],"payload":{"lines":"46,48"}}],"payload":{"lines":"40,41"}},{"content":"📚 Chapter 7: Retrieval Augmented Generation: Harnessing External Knowledge","children":[{"content":"Harnessing external knowledge 🧠🌐","children":[],"payload":{"lines":"50,51"}},{"content":"Overcoming knowledge cutoff 🚧❌","children":[],"payload":{"lines":"51,52"}},{"content":"Enhancing accuracy and relevance 💯","children":[],"payload":{"lines":"52,53"}},{"content":"Personalizing responses 🧑‍🤝‍🧑","children":[],"payload":{"lines":"53,55"}}],"payload":{"lines":"48,49"}},{"content":"👐 Chapter 8: Hugging Face: The Gateway to Open Source LLMs","children":[{"content":"Gateway to open source LLMs 🚪","children":[],"payload":{"lines":"57,58"}},{"content":"Hugging Face Hub: model repository 📚🤖","children":[],"payload":{"lines":"58,59"}},{"content":"Transformers library: powerful toolkit 🧰","children":[],"payload":{"lines":"59,60"}},{"content":"Open source ethos: collaboration and transparency 🤝","children":[],"payload":{"lines":"60,62"}}],"payload":{"lines":"55,56"}},{"content":"💻  Chapter 9: Local Model Deployment: Unleashing the Power of Offline Models","children":[{"content":"Offline models: power and independence 💪🔌","children":[],"payload":{"lines":"64,65"}},{"content":"Offline accessibility 📵","children":[],"payload":{"lines":"65,66"}},{"content":"Enhanced privacy and security 🔐","children":[],"payload":{"lines":"66,67"}},{"content":"Reduced latency ⚡","children":[],"payload":{"lines":"67,68"}},{"content":"Customization and control 🎛️","children":[],"payload":{"lines":"68,70"}}],"payload":{"lines":"62,63"}},{"content":"🎯 Chapter 10: Fine-Tuning for Specialization: Adapting LLMs to Your Domain","children":[{"content":"Adapting LLMs to your domain 🤖➡️🧑‍🔬","children":[],"payload":{"lines":"72,73"}},{"content":"Improved accuracy 💯","children":[],"payload":{"lines":"73,74"}},{"content":"Enhanced fluency 🗣️","children":[],"payload":{"lines":"74,75"}},{"content":"Reduced data requirements 📉","children":[],"payload":{"lines":"75,77"}}],"payload":{"lines":"70,71"}},{"content":"🚀 Chapter 11: Performance Optimization: Maximizing Efficiency with Quantization and GPUs","children":[{"content":"Efficiency with quantization and GPUs ⚡🧠","children":[],"payload":{"lines":"79,80"}},{"content":"Quantization: a diet for digital brains 🤖🍽️","children":[],"payload":{"lines":"80,81"}},{"content":"GPU acceleration: parallel processing 💨","children":[],"payload":{"lines":"81,82"}},{"content":"Memory optimization 🧠🗄️","children":[],"payload":{"lines":"82,84"}}],"payload":{"lines":"77,78"}},{"content":"🕷️ Chapter 12: Web Scraping with Vision AI: Extracting Data from the Visual World","children":[{"content":"Extracting data from the visual world 👁️🌐","children":[],"payload":{"lines":"86,87"}},{"content":"GPT-4 Vision: language and vision 🧠👁️","children":[],"payload":{"lines":"87,88"}},{"content":"Puppeteer: browser automation 🤖💻","children":[],"payload":{"lines":"88,89"}},{"content":"Beyond text extraction 📈📊","children":[],"payload":{"lines":"89,91"}}],"payload":{"lines":"84,85"}},{"content":"🧱 Chapter 13: Building Your Own Tokenizer: A Deep Dive into Byte Pair Encoding","children":[{"content":"Deep dive into Byte Pair Encoding 🤿","children":[],"payload":{"lines":"93,94"}},{"content":"Merging bytes for efficiency 🗜️","children":[],"payload":{"lines":"94,95"}},{"content":"Encoding and decoding 🔁","children":[],"payload":{"lines":"95,96"}},{"content":"Fine-tuning the tokenizer 🛠️","children":[],"payload":{"lines":"96,98"}}],"payload":{"lines":"91,92"}},{"content":"💻 Chapter 14: Creating a Code Interpreter: Empowering LLMs with Python Execution","children":[{"content":"Empowering LLMs with Python 🐍🤖","children":[],"payload":{"lines":"100,101"}},{"content":"Function calling 🧰","children":[],"payload":{"lines":"101,102"}},{"content":"Code execution ⚙️","children":[],"payload":{"lines":"102,103"}},{"content":"Data analysis and visualization 📊","children":[],"payload":{"lines":"103,104"}},{"content":"Real-world interactions 🌎","children":[],"payload":{"lines":"104,106"}}],"payload":{"lines":"98,99"}},{"content":"🤖🌎  Chapter 15: Multimodal AI: The Future of Language Models","children":[{"content":"The future of language models 🚀","children":[],"payload":{"lines":"108,109"}},{"content":"Expanding the LLM's sensory palette 🎨🎶","children":[],"payload":{"lines":"109,110"}},{"content":"Multimodal architectures 🤝🧠","children":[],"payload":{"lines":"110,111"}},{"content":"Tokenizing the multimodal world 🌎🔢","children":[],"payload":{"lines":"111,113"}}],"payload":{"lines":"106,107"}},{"content":"🤖💪 Chapter 16: Autonomous Agents: LLMs that Act Independently","children":[{"content":"LLMs that act independently 🧠🚀","children":[],"payload":{"lines":"115,116"}},{"content":"Planning, action, and learning 🧭⚙️📚","children":[],"payload":{"lines":"116,117"}},{"content":"Personalized assistants 🧑‍💼🤖","children":[],"payload":{"lines":"117,118"}},{"content":"Automated research and discovery 🔬🤖","children":[],"payload":{"lines":"118,119"}},{"content":"Creative collaboration 🎨🤖","children":[],"payload":{"lines":"119,121"}}],"payload":{"lines":"113,114"}},{"content":"🧭 Chapter 17: The Ethical Landscape: Navigating Bias, Hallucinations, and Responsible AI","children":[{"content":"Navigating bias, hallucinations, and responsible AI 🚧🧠","children":[],"payload":{"lines":"123,124"}},{"content":"Bias: unmasking hidden prejudices 🎭","children":[],"payload":{"lines":"124,125"}},{"content":"Hallucinations: fabricated truths 👻","children":[],"payload":{"lines":"125,126"}},{"content":"Responsible AI development 🦺","children":[],"payload":{"lines":"126,127"}},{"content":"Ethical awareness 😇","children":[],"payload":{"lines":"127,131"}}],"payload":{"lines":"121,122"}}],"payload":{"lines":"1,2"}},{"initialExpandLevel":2})</script>
</body>
</html>
