<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.1-alpha.4/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:q,mm:v}=window,j=new q.Toolbar;j.attach(v);const we=j.render();we.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(we)})})()</script><script>((f,d,h,u)=>{const g=f();window.mm=g.Markmap.create("svg#mindmap",(d||g.deriveOptions)(u),h)})(()=>window.markmap,null,{"content":"ğŸ“š Hacker's Guide to LLMs","children":[{"content":"ğŸ§  Chapter 1: The Rise of LLMs: From Calculators to Creative Machines","children":[{"content":"From calculators to creative machines ğŸ§®â¡ï¸ğŸ¨","children":[],"payload":{"lines":"5,6"}},{"content":"AI revolution ğŸ’¥","children":[],"payload":{"lines":"6,7"}},{"content":"Transformer architecture ğŸ¤–","children":[],"payload":{"lines":"7,9"}}],"payload":{"lines":"3,4"}},{"content":"ğŸ§  Chapter 2: Understanding the Transformer: Attention is All You Need","children":[{"content":"Attention is all you need ğŸ‘€","children":[],"payload":{"lines":"11,12"}},{"content":"Input embeddings: words to numbers ğŸ”¤â¡ï¸ğŸ”¢","children":[],"payload":{"lines":"12,13"}},{"content":"Positional encodings: preserving order ğŸ”¢â¡ï¸ğŸ—ºï¸","children":[],"payload":{"lines":"13,14"}},{"content":"Self-attention: word relationships ğŸ”—","children":[],"payload":{"lines":"14,15"}},{"content":"Multi-head attention: diverse perspectives ğŸ§ ğŸ§ ğŸ§ ","children":[],"payload":{"lines":"15,17"}}],"payload":{"lines":"9,10"}},{"content":"ğŸ‹ï¸â€â™€ï¸ Chapter 3: The Training Pipeline: Pre-Training, Fine-Tuning, and Beyond","children":[{"content":"Pre-training:  ocean of data ğŸŒŠğŸ“š","children":[],"payload":{"lines":"19,20"}},{"content":"Fine-tuning: specific tasks ğŸ¯","children":[],"payload":{"lines":"20,21"}},{"content":"Instruction tuning: following instructions ğŸ“ğŸ¤–","children":[],"payload":{"lines":"21,22"}},{"content":"RLHF: human feedback ğŸ§‘â€ğŸ«ğŸ¤–","children":[],"payload":{"lines":"22,24"}}],"payload":{"lines":"17,18"}},{"content":"âœï¸ Chapter 4: The Art of Prompt Engineering: Guiding Your LLM to Success","children":[{"content":"Guiding your LLM ğŸ§‘â€âœˆï¸ğŸ¤–","children":[],"payload":{"lines":"26,27"}},{"content":"Clarity and conciseness ğŸ’","children":[],"payload":{"lines":"27,28"}},{"content":"Specificity ğŸ¯","children":[],"payload":{"lines":"28,29"}},{"content":"Context is key ğŸ—ºï¸","children":[],"payload":{"lines":"29,30"}},{"content":"Iterative refinement ğŸ”„","children":[],"payload":{"lines":"30,32"}}],"payload":{"lines":"24,25"}},{"content":"ğŸ”Œ Chapter 5: Interacting with OpenAI API: Accessing GPT Power Programmatically","children":[{"content":"Accessing GPT power ğŸ§ âš¡","children":[],"payload":{"lines":"34,35"}},{"content":"Programmatic prompts ğŸ¤–ğŸ’»","children":[],"payload":{"lines":"35,36"}},{"content":"Function calling ğŸ§°","children":[],"payload":{"lines":"36,37"}},{"content":"Fine-tuning ğŸ› ï¸","children":[],"payload":{"lines":"37,38"}},{"content":"Embeddings ğŸ§¬","children":[],"payload":{"lines":"38,40"}}],"payload":{"lines":"32,33"}},{"content":"ğŸ§° Chapter 6: Extending LLM Capabilities: Function Calling and Tool Integration","children":[{"content":"Function calling: language to action ğŸ’¬â¡ï¸ğŸ’ª","children":[],"payload":{"lines":"42,43"}},{"content":"Tool integration ğŸ”ŒğŸ§°","children":[],"payload":{"lines":"43,44"}},{"content":"Real-time information âš¡ï¸","children":[],"payload":{"lines":"44,45"}},{"content":"Data manipulation ğŸ§®","children":[],"payload":{"lines":"45,46"}},{"content":"Complex actions ğŸš€","children":[],"payload":{"lines":"46,48"}}],"payload":{"lines":"40,41"}},{"content":"ğŸ“š Chapter 7: Retrieval Augmented Generation: Harnessing External Knowledge","children":[{"content":"Harnessing external knowledge ğŸ§ ğŸŒ","children":[],"payload":{"lines":"50,51"}},{"content":"Overcoming knowledge cutoff ğŸš§âŒ","children":[],"payload":{"lines":"51,52"}},{"content":"Enhancing accuracy and relevance ğŸ’¯","children":[],"payload":{"lines":"52,53"}},{"content":"Personalizing responses ğŸ§‘â€ğŸ¤â€ğŸ§‘","children":[],"payload":{"lines":"53,55"}}],"payload":{"lines":"48,49"}},{"content":"ğŸ‘ Chapter 8: Hugging Face: The Gateway to Open Source LLMs","children":[{"content":"Gateway to open source LLMs ğŸšª","children":[],"payload":{"lines":"57,58"}},{"content":"Hugging Face Hub: model repository ğŸ“šğŸ¤–","children":[],"payload":{"lines":"58,59"}},{"content":"Transformers library: powerful toolkit ğŸ§°","children":[],"payload":{"lines":"59,60"}},{"content":"Open source ethos: collaboration and transparency ğŸ¤","children":[],"payload":{"lines":"60,62"}}],"payload":{"lines":"55,56"}},{"content":"ğŸ’»  Chapter 9: Local Model Deployment: Unleashing the Power of Offline Models","children":[{"content":"Offline models: power and independence ğŸ’ªğŸ”Œ","children":[],"payload":{"lines":"64,65"}},{"content":"Offline accessibility ğŸ“µ","children":[],"payload":{"lines":"65,66"}},{"content":"Enhanced privacy and security ğŸ”","children":[],"payload":{"lines":"66,67"}},{"content":"Reduced latency âš¡","children":[],"payload":{"lines":"67,68"}},{"content":"Customization and control ğŸ›ï¸","children":[],"payload":{"lines":"68,70"}}],"payload":{"lines":"62,63"}},{"content":"ğŸ¯ Chapter 10: Fine-Tuning for Specialization: Adapting LLMs to Your Domain","children":[{"content":"Adapting LLMs to your domain ğŸ¤–â¡ï¸ğŸ§‘â€ğŸ”¬","children":[],"payload":{"lines":"72,73"}},{"content":"Improved accuracy ğŸ’¯","children":[],"payload":{"lines":"73,74"}},{"content":"Enhanced fluency ğŸ—£ï¸","children":[],"payload":{"lines":"74,75"}},{"content":"Reduced data requirements ğŸ“‰","children":[],"payload":{"lines":"75,77"}}],"payload":{"lines":"70,71"}},{"content":"ğŸš€ Chapter 11: Performance Optimization: Maximizing Efficiency with Quantization and GPUs","children":[{"content":"Efficiency with quantization and GPUs âš¡ğŸ§ ","children":[],"payload":{"lines":"79,80"}},{"content":"Quantization: a diet for digital brains ğŸ¤–ğŸ½ï¸","children":[],"payload":{"lines":"80,81"}},{"content":"GPU acceleration: parallel processing ğŸ’¨","children":[],"payload":{"lines":"81,82"}},{"content":"Memory optimization ğŸ§ ğŸ—„ï¸","children":[],"payload":{"lines":"82,84"}}],"payload":{"lines":"77,78"}},{"content":"ğŸ•·ï¸ Chapter 12: Web Scraping with Vision AI: Extracting Data from the Visual World","children":[{"content":"Extracting data from the visual world ğŸ‘ï¸ğŸŒ","children":[],"payload":{"lines":"86,87"}},{"content":"GPT-4 Vision: language and vision ğŸ§ ğŸ‘ï¸","children":[],"payload":{"lines":"87,88"}},{"content":"Puppeteer: browser automation ğŸ¤–ğŸ’»","children":[],"payload":{"lines":"88,89"}},{"content":"Beyond text extraction ğŸ“ˆğŸ“Š","children":[],"payload":{"lines":"89,91"}}],"payload":{"lines":"84,85"}},{"content":"ğŸ§± Chapter 13: Building Your Own Tokenizer: A Deep Dive into Byte Pair Encoding","children":[{"content":"Deep dive into Byte Pair Encoding ğŸ¤¿","children":[],"payload":{"lines":"93,94"}},{"content":"Merging bytes for efficiency ğŸ—œï¸","children":[],"payload":{"lines":"94,95"}},{"content":"Encoding and decoding ğŸ”","children":[],"payload":{"lines":"95,96"}},{"content":"Fine-tuning the tokenizer ğŸ› ï¸","children":[],"payload":{"lines":"96,98"}}],"payload":{"lines":"91,92"}},{"content":"ğŸ’» Chapter 14: Creating a Code Interpreter: Empowering LLMs with Python Execution","children":[{"content":"Empowering LLMs with Python ğŸğŸ¤–","children":[],"payload":{"lines":"100,101"}},{"content":"Function calling ğŸ§°","children":[],"payload":{"lines":"101,102"}},{"content":"Code execution âš™ï¸","children":[],"payload":{"lines":"102,103"}},{"content":"Data analysis and visualization ğŸ“Š","children":[],"payload":{"lines":"103,104"}},{"content":"Real-world interactions ğŸŒ","children":[],"payload":{"lines":"104,106"}}],"payload":{"lines":"98,99"}},{"content":"ğŸ¤–ğŸŒ  Chapter 15: Multimodal AI: The Future of Language Models","children":[{"content":"The future of language models ğŸš€","children":[],"payload":{"lines":"108,109"}},{"content":"Expanding the LLM's sensory palette ğŸ¨ğŸ¶","children":[],"payload":{"lines":"109,110"}},{"content":"Multimodal architectures ğŸ¤ğŸ§ ","children":[],"payload":{"lines":"110,111"}},{"content":"Tokenizing the multimodal world ğŸŒğŸ”¢","children":[],"payload":{"lines":"111,113"}}],"payload":{"lines":"106,107"}},{"content":"ğŸ¤–ğŸ’ª Chapter 16: Autonomous Agents: LLMs that Act Independently","children":[{"content":"LLMs that act independently ğŸ§ ğŸš€","children":[],"payload":{"lines":"115,116"}},{"content":"Planning, action, and learning ğŸ§­âš™ï¸ğŸ“š","children":[],"payload":{"lines":"116,117"}},{"content":"Personalized assistants ğŸ§‘â€ğŸ’¼ğŸ¤–","children":[],"payload":{"lines":"117,118"}},{"content":"Automated research and discovery ğŸ”¬ğŸ¤–","children":[],"payload":{"lines":"118,119"}},{"content":"Creative collaboration ğŸ¨ğŸ¤–","children":[],"payload":{"lines":"119,121"}}],"payload":{"lines":"113,114"}},{"content":"ğŸ§­ Chapter 17: The Ethical Landscape: Navigating Bias, Hallucinations, and Responsible AI","children":[{"content":"Navigating bias, hallucinations, and responsible AI ğŸš§ğŸ§ ","children":[],"payload":{"lines":"123,124"}},{"content":"Bias: unmasking hidden prejudices ğŸ­","children":[],"payload":{"lines":"124,125"}},{"content":"Hallucinations: fabricated truths ğŸ‘»","children":[],"payload":{"lines":"125,126"}},{"content":"Responsible AI development ğŸ¦º","children":[],"payload":{"lines":"126,127"}},{"content":"Ethical awareness ğŸ˜‡","children":[],"payload":{"lines":"127,131"}}],"payload":{"lines":"121,122"}}],"payload":{"lines":"1,2"}},{"initialExpandLevel":2})</script>
</body>
</html>
